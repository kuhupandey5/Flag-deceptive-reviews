{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# FLAG DECEPTIVE REVIEWS"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# Global Import\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as py\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from datetime import datetime, date, time\n",
    "from sklearn.svm import LinearSVC\n",
    "from string import punctuation\n",
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "import requests\n",
    "import string\n",
    "import numpy\n",
    "import os\n",
    "import pandas as pd\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "import json\n",
    "%matplotlib inline\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kuhupandey/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/kuhupandey/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/kuhupandey/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "def dataset_join(path,polarity):\n",
    "    df = pd.DataFrame(columns=['text','deceptive'])\n",
    "    for directory in os.listdir(path):\n",
    "        sub_directory = os.path.join(path, directory)\n",
    "        if os.path.isdir(sub_directory):\n",
    "            for filename in os.listdir(sub_directory):\n",
    "                if not filename.startswith('.'):\n",
    "                    sub_directory2 = os.path.join(sub_directory, filename)\n",
    "                    for txt_file in os.listdir(sub_directory2):\n",
    "                        with open(os.path.join(sub_directory2, txt_file)) as f:\n",
    "                            txt_file = f.read()\n",
    "                            current_df = pd.DataFrame({'text': [txt_file],'deceptive':polarity})\n",
    "                            df = df.append(current_df, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def text_cleaning(text):\n",
    "    '''\n",
    "    Make text lowercase, remove text in square brackets,remove links,remove special characters\n",
    "    and remove words containing numbers.\n",
    "    '''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub(\"\\\\W\",\" \",text) # remove special chars\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_content_based_features(input_data): \n",
    "    length = [len(data) for data in input_data]\n",
    "    tokens = [nltk.word_tokenize(data) for data in input_data]\n",
    "    taggeds = [nltk.pos_tag(token,tagset='universal') for token in tokens] \n",
    "    tag_fds = [nltk.FreqDist(tag for (word, tag) in tagged) for tagged in taggeds]\n",
    "    verb_minus_noun = [tag_fd['VERB'] - tag_fd['NOUN'] for tag_fd in tag_fds]\n",
    "    punctuation_amount = [len(re.findall('['+punctuation+']', data)) for data in input_data]\n",
    "    features = {'punctuation_amount': punctuation_amount,'verb_minus_noun': verb_minus_noun}\n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "def extract_ngram_1_3_features(tfidVect,input_data):\n",
    "    XInput = TfidfVect.transform(input_data).toarray()\n",
    "    return pd.DataFrame(XInput)\n",
    "\n",
    "def extract_text_features(tfidVect, input_data):\n",
    "    df1= extract_content_based_features(input_data)\n",
    "    df2= extract_ngram_1_3_features(tfidVect,input_data)\n",
    "    ## extract_ngram (1 3) features\n",
    "    result = pd.concat([df1, df2], axis=1, join='inner')\n",
    "    return result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "df_negative = dataset_join('op_spam_v1.4/negative_polarity', 1)\n",
    "df_positive = dataset_join('op_spam_v1.4/positive_polarity', 0)\n",
    "\n",
    "df_opinion = df_negative.append(df_positive, ignore_index=True)\n",
    "\n",
    "df_opinion['text']=df_opinion['text'].apply(text_cleaning)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# define the TfidfVect. \n",
    "min_df=3\n",
    "max_df=0.96\n",
    "# ngram (1,3) gives better percsion than 1,2 and 1,4 doesn't improve accuracy. \n",
    "ngram_range = (1,3) \n",
    "TfidfVect= TfidfVectorizer(min_df = min_df, max_df = max_df, ngram_range = ngram_range)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# split the dataset in training and test set:\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    df_opinion['text'], df_opinion['deceptive'].values, test_size=0.10, random_state=None)\n",
    "\n",
    "TfidfVect.fit(docs_train)\n",
    "# define featuers from yelp user/business meta-data. \n",
    "\n",
    "Xtrain=extract_text_features(TfidfVect, docs_train)\n",
    "Xtest=extract_text_features(TfidfVect, docs_test)\n",
    "clf  = LinearSVC(C=100)\n",
    "clf.fit(Xtrain, y_train.astype('int'))\n",
    "y_predicted = clf.predict(Xtest)\n",
    "\n",
    "print(\"**Classification Report**\\n\")\n",
    "print(metrics.classification_report(y_test.astype('int'), y_predicted))\n",
    "print(\"**Confusion Matrix**\\n\")\n",
    "cm = metrics.confusion_matrix(y_test.astype('int'), y_predicted)\n",
    "print(cm)\n",
    "\n",
    "plt.matshow(cm)\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "**Classification Report**\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.94        77\n",
      "           1       0.94      0.95      0.95        83\n",
      "\n",
      "    accuracy                           0.94       160\n",
      "   macro avg       0.94      0.94      0.94       160\n",
      "weighted avg       0.94      0.94      0.94       160\n",
      "\n",
      "**Confusion Matrix**\n",
      "\n",
      "[[72  5]\n",
      " [ 4 79]]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/kuhupandey/opt/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "def gen(file_name):\n",
    "    with open(file_name) as fh:\n",
    "        line = fh.readline()\n",
    "        while line:\n",
    "            yield json.loads(line)\n",
    "            line = fh.readline()\n",
    "\n",
    "def build_csv_dataset(path, file_name, n_lines, fields):\n",
    "    \n",
    "    line_gen = gen(path)\n",
    "    \n",
    "    with open(f'{file_name}.csv', 'w') as f:\n",
    "        csv_writer = csv.writer(f)\n",
    "        \n",
    "        i = 0\n",
    "        for line in line_gen:\n",
    "            if i == 0:\n",
    "                csv_writer.writerow(fields)\n",
    "                i += 1\n",
    "            row = {k: line[k] for k in line.keys() if k in fields}.values()\n",
    "            csv_writer.writerow(row)\n",
    "            i += 1\n",
    "            \n",
    "            if i % 1000 == 0 :\n",
    "                print(f'Processed {i}/{n_lines} lines', end=\"\\r\")\n",
    "\n",
    "            if i == n_lines:\n",
    "                break\n",
    "        print('all done!')    \n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "build_csv_dataset('yelp_dataset/yelp_academic_dataset_review.json','reviews',100000, ('user_id', 'business_id', 'stars', 'text'))\n",
    "build_csv_dataset('yelp_dataset/yelp_academic_dataset_business.json','business',100000,('business_id','name','stars','review_count'))\n",
    "build_csv_dataset('yelp_dataset/yelp_academic_dataset_user.json','user',100000,('user_id','review_count',\"friends\",\"average_stars\"))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "all done! 100000/100000 lines\n",
      "all done! 100000/100000 lines\n",
      "all done! 100000/100000 lines\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "#reviews_file = 'mass_yelp_academic_dataset_resturants_reviews.json' \n",
    "reviews_file = 'yelp_dataset/yelp_academic_dataset_review.json'\n",
    "business_file = 'yelp_dataset/yelp_academic_dataset_business.json'\n",
    "user_file = 'yelp_dataset/yelp_academic_dataset_user.json'\n",
    "\n",
    "reviews_df = pd.read_csv(\"reviews.csv\")\n",
    "business_df =  pd.read_csv(\"business.csv\")\n",
    "user_df =  pd.read_csv(\"user.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "business_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6iYb2HFDywm3zjuRg0shjw</td>\n",
       "      <td>Oskar Blues Taproom</td>\n",
       "      <td>4.0</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tCbdrRPZA0oiIYSmHG3J0w</td>\n",
       "      <td>Flying Elephants at PDX</td>\n",
       "      <td>4.0</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bvN78flM8NLprQ1a1y5dRg</td>\n",
       "      <td>The Reclaimory</td>\n",
       "      <td>4.5</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oaepsyvc0J17qwi8cfrOWg</td>\n",
       "      <td>Great Clips</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PE9uqAjdw0E4-8mjGl3wVA</td>\n",
       "      <td>Crossfit Terminus</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                     name  stars  review_count\n",
       "0  6iYb2HFDywm3zjuRg0shjw      Oskar Blues Taproom    4.0            86\n",
       "1  tCbdrRPZA0oiIYSmHG3J0w  Flying Elephants at PDX    4.0           126\n",
       "2  bvN78flM8NLprQ1a1y5dRg           The Reclaimory    4.5            13\n",
       "3  oaepsyvc0J17qwi8cfrOWg              Great Clips    3.0             8\n",
       "4  PE9uqAjdw0E4-8mjGl3wVA        Crossfit Terminus    4.0            14"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "## prepare the metadata for merge. \n",
    "user_df.rename(columns={'review_count': 'user_review_count'}, inplace=True)\n",
    "business_df.rename(columns={'review_count': 'business_review_count','stars':'business_stars'}, inplace=True)\n",
    "\n",
    "## filter out negative or netural reviews, \n",
    "reviews_df=reviews_df[reviews_df['stars']>=4]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "def extract_metadata_featuers(input_reviews):\n",
    "    #fill the user and buiness data from other data frames\n",
    "    merged_data=pd.concat([input_reviews,business_df, user_df], axis=1, join='inner')\n",
    "    # replace the list of friends with friends count.\n",
    "    merged_data['friends']  = merged_data['friends'].apply(lambda x: len(x))\n",
    "    # extract the interesting data only\n",
    "    return merged_data[['user_review_count','average_stars','friends','business_review_count','business_stars']]\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "from sklearn.covariance import EllipticEnvelope\n",
    "# Now that we filter out the negative reviews and select the interesting featuers from user/business metadat, it is time\n",
    "# to train the outlier detector model. \n",
    "anomaly_detector = EllipticEnvelope(contamination=0.15)\n",
    "\n",
    "#ad_docs_train, ad_docs_test = train_test_split(df, test_size=0.10, random_state=None)\n",
    "anomaly_detector.fit(extract_metadata_featuers(reviews_df))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "EllipticEnvelope(contamination=0.15)"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "def offline_outlier_detector_then_clf(input_data):\n",
    "    f = extract_metadata_featuers(input_data)\n",
    "    ad_result = anomaly_detector.predict(f)\n",
    "    Xinput=extract_text_features(TfidfVect, input_data['text'])\n",
    "    clf_result = clf.predict(Xtest)\n",
    "    return [clf_result if (ad_result==-1) else 1 for clf_result,ad_result in zip(clf_result,ad_result)]\n",
    "    \n",
    "offline_outlier_detector_then_clf(reviews_df[1:4])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1, 1, 1]"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "from urllib.parse import quote\n",
    "# API constants\n",
    "API_HOST = 'https://api.yelp.com'\n",
    "SEARCH_PATH = '/v3/businesses/search'\n",
    "BUSINESS_PATH = '/v3/businesses/'\n",
    "api_key = \"yNmgmV9jq-RFzNW_0PfPJ-TFBJ9d3CWrI1VHuv2qT7ai9rlBBPq7HviMCmpA9gJEtSAN9bjMFEkr0Cm_yyeMuaAD4WZL62-DU08HmSyFpSmeCHo5-vwlIekS_0uwYXYx\"\n",
    "SEARCH_LIMIT = 1\n",
    "\n",
    "def request(host, path, api_key, url_params=None):\n",
    "    \"\"\"Given your API_KEY, send a GET request to the API.\n",
    "    Args:\n",
    "        host (str): The domain host of the API.\n",
    "        path (str): The path of the API after the domain.\n",
    "        API_KEY (str): Your API Key.\n",
    "        url_params (dict): An optional set of query parameters in the request.\n",
    "    Returns:\n",
    "        dict: The JSON response from the request.\n",
    "    Raises:\n",
    "        HTTPError: An error occurs from the HTTP request.\n",
    "    \"\"\"\n",
    "    url_params = url_params or {}\n",
    "    url = '{0}{1}'.format(host, quote(path.encode('utf8')))\n",
    "    headers = {\n",
    "        'Authorization': 'Bearer %s' % api_key,\n",
    "    }\n",
    "\n",
    "    print(u'Querying {0} ...'.format(url))\n",
    "\n",
    "    response = requests.request('GET', url, headers=headers, params=url_params)\n",
    "\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def get_business(term, location):\n",
    "    \"\"\"Query the Search API by a search term and location.\n",
    "    Args:\n",
    "        term (str): The search term passed to the API.\n",
    "        location (str): The search location passed to the API.\n",
    "    Returns:\n",
    "        dict: The JSON response from the request.\n",
    "    \"\"\"\n",
    "\n",
    "    url_params = {\n",
    "        'term': term.replace(' ', '+'),\n",
    "        'location': location.replace(' ', '+'),\n",
    "        'limit': SEARCH_LIMIT\n",
    "    }\n",
    "    request_1 = request(API_HOST, SEARCH_PATH, api_key, url_params=url_params)\n",
    "    business_id = request_1['businesses'][0]['id']\n",
    "    request_2 = request(API_HOST, BUSINESS_PATH + business_id+\"/reviews\", api_key)\n",
    "    return {'review':request_2['reviews'][0]['text'], 'rating':request_2['reviews'][0]['rating'], \n",
    "           'review_count':request_1['businesses'][0]['review_count'], 'business_stars':request_1['businesses'][0]['rating']}\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "# test, fetch cider mill first review.\n",
    "\n",
    "response=get_business('cider mill','syracuse')\n",
    "response['review']"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Querying https://api.yelp.com/v3/businesses/search ...\n",
      "Querying https://api.yelp.com/v3/businesses/VhUGLVBGu9aDMLdf39yx0A/reviews ...\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'By far one of the most friendliest and attentive brewery I have been to. I believe the owner was the one checking on everyone and even brought water bowls...'"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "def online_detector_using_rest_api(business, location):\n",
    "    response= get_business(business, location)\n",
    "    business_review_count = response['review_count']\n",
    "    business_stars = response['business_stars']\n",
    "    text=response['review']\n",
    "    rating=response['rating']\n",
    "    print()\n",
    "    print(f'extracted the review {text}')\n",
    "    # YELP doesn't allow fetching user friends count,review_count, use 1, later use CRAWL to get that number from html page,.\n",
    "    count = 1\n",
    "    features_=pd.DataFrame({'user_review_count' :[count,0], 'average_stars':[rating,0],'friends':[count,0],'business_review_count':[business_review_count,0],'business_stars':[business_stars,0]})\n",
    "    ad_result = anomaly_detector.predict(features_)\n",
    "    # if not outlier, return real\n",
    "    if ad_result[0]==1:\n",
    "        return f\"review is truthful\"\n",
    "    else:\n",
    "        # Outlier, check using text classifer. \n",
    "        #vectorize the review text.\n",
    "        Xinput_=extract_text_features(TfidfVect, [text])\n",
    "        clf_result = clf.predict(Xinput_)\n",
    "        return f\"review is truthful\" if clf_result[0]==1 else f\"review is deceptive\"\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "online_detector_using_rest_api('cider mill','syracuse')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Querying https://api.yelp.com/v3/businesses/search ...\n",
      "Querying https://api.yelp.com/v3/businesses/VhUGLVBGu9aDMLdf39yx0A/reviews ...\n",
      "\n",
      "extracted the review By far one of the most friendliest and attentive brewery I have been to. I believe the owner was the one checking on everyone and even brought water bowls...\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'review is truthful'"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#Future implementation"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def deceptive_checker_1(text):\n",
    "    url = 'http://fake-review-detector'\n",
    "    payload = {'review_text': text}\n",
    "    r = requests.post(url, data=payload)\n",
    "    return r.text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "review1='Best restaurant. Charlie is extremely nice and friendly. I recommend all the food, especially chicken  plate and sandwich. You wont get disappointed'\n",
    "deceptive_checker_1(review1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}